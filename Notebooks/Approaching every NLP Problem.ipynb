{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e9702871",
   "metadata": {},
   "source": [
    "# Approaching (almost) every NLP Problem\n",
    "\n",
    "This is my first notebook for Data Science, for starter I will write some simple models and standard feature extraction.\n",
    "\n",
    "## I Importing Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c35355c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import xgboost as xgb\n",
    "from tqdm import tqdm\n",
    "from sklearn.svm import SVC\n",
    "from keras.models import Sequential\n",
    "from keras.layers.recurrent import LSTM, GRU\n",
    "from keras.layers.core import Dense, Activation, Dropout\n",
    "from keras.layers.embeddings import Embedding\n",
    "from keras.layers.normalization import batch_normalization\n",
    "from keras.utils import np_utils\n",
    "from sklearn import model_selection, decomposition, preprocessing, metrics, pipeline\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from keras.layers import GlobalAveragePooling1D, Conv1D, MaxPooling1D, Flatten, SpatialDropout1D\n",
    "from keras.preprocessing import sequence, text\n",
    "from keras.callbacks import EarlyStopping\n",
    "import nltk\n",
    "from nltk import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "stop_words = stopwords.words('english')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34b86035",
   "metadata": {},
   "source": [
    "## II Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "99d11973",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>text</th>\n",
       "      <th>author</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>id26305</td>\n",
       "      <td>This process, however, afforded me no means of...</td>\n",
       "      <td>EAP</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>id17569</td>\n",
       "      <td>It never once occurred to me that the fumbling...</td>\n",
       "      <td>HPL</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>id11008</td>\n",
       "      <td>In his left hand was a gold snuff box, from wh...</td>\n",
       "      <td>EAP</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>id27763</td>\n",
       "      <td>How lovely is spring As we looked from Windsor...</td>\n",
       "      <td>MWS</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>id12958</td>\n",
       "      <td>Finding nothing else, not even gold, the Super...</td>\n",
       "      <td>HPL</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        id                                               text author\n",
       "0  id26305  This process, however, afforded me no means of...    EAP\n",
       "1  id17569  It never once occurred to me that the fumbling...    HPL\n",
       "2  id11008  In his left hand was a gold snuff box, from wh...    EAP\n",
       "3  id27763  How lovely is spring As we looked from Windsor...    MWS\n",
       "4  id12958  Finding nothing else, not even gold, the Super...    HPL"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train = pd.read_csv(\"../../data/spooky/train.csv\")\n",
    "test = pd.read_csv(\"../../data/spooky/test.csv\")\n",
    "sample = pd.read_csv(\"../../data/spooky/sample_submission.csv\")\n",
    "\n",
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9b124a30",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>id02310</td>\n",
       "      <td>Still, as I urged our leaving Ireland with suc...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>id24541</td>\n",
       "      <td>If a fire wanted fanning, it could readily be ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>id00134</td>\n",
       "      <td>And when they had broken down the frail door t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>id27757</td>\n",
       "      <td>While I was thinking how I should possibly man...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>id04081</td>\n",
       "      <td>I am not sure to what limit his knowledge may ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        id                                               text\n",
       "0  id02310  Still, as I urged our leaving Ireland with suc...\n",
       "1  id24541  If a fire wanted fanning, it could readily be ...\n",
       "2  id00134  And when they had broken down the frail door t...\n",
       "3  id27757  While I was thinking how I should possibly man...\n",
       "4  id04081  I am not sure to what limit his knowledge may ..."
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9f4b3e37",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>EAP</th>\n",
       "      <th>HPL</th>\n",
       "      <th>MWS</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>id02310</td>\n",
       "      <td>0.403494</td>\n",
       "      <td>0.287808</td>\n",
       "      <td>0.308698</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>id24541</td>\n",
       "      <td>0.403494</td>\n",
       "      <td>0.287808</td>\n",
       "      <td>0.308698</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>id00134</td>\n",
       "      <td>0.403494</td>\n",
       "      <td>0.287808</td>\n",
       "      <td>0.308698</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>id27757</td>\n",
       "      <td>0.403494</td>\n",
       "      <td>0.287808</td>\n",
       "      <td>0.308698</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>id04081</td>\n",
       "      <td>0.403494</td>\n",
       "      <td>0.287808</td>\n",
       "      <td>0.308698</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        id       EAP       HPL       MWS\n",
       "0  id02310  0.403494  0.287808  0.308698\n",
       "1  id24541  0.403494  0.287808  0.308698\n",
       "2  id00134  0.403494  0.287808  0.308698\n",
       "3  id27757  0.403494  0.287808  0.308698\n",
       "4  id04081  0.403494  0.287808  0.308698"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c19257a",
   "metadata": {},
   "source": [
    "## III Multiclass logloss evaluation metric by Kaggle\n",
    "In the next cell, I will write the code of the multiclass logloss evaluation metric given by kaggle for this specific competition (spooky)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9c8d5591",
   "metadata": {},
   "outputs": [],
   "source": [
    "def multiclass_logloss(actual, predicted, eps=1e-15):\n",
    "    if len(actual.shape) == 1:\n",
    "        actual2 = np.zeros((actual.shape[0], predicted.shape[1]))\n",
    "        for i, val in enumerate(actual):\n",
    "            actual2[i, val] = 1\n",
    "        actual = actual2\n",
    "        \n",
    "    clip = np.clip(predicted, eps, 1 - eps)\n",
    "    rows = actual.shape[0]\n",
    "    vsota = np.sum(actual * np.log(clip))\n",
    "\n",
    "    return -1.0 / rows * vsota"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7827e877",
   "metadata": {},
   "source": [
    "## IV Convert text labels to numerical labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6418f51c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 1 0 2 1 2 0 0 0 2 2 0 1 1 0 2 0 2 0 1 0 1 0 0 0 0 0 0 1 1 0 2 2 1 1 1 1\n",
      " 2 1 0 1 1 1 2 0 0 2 1 1 0]\n"
     ]
    }
   ],
   "source": [
    "lbl_enc = preprocessing.LabelEncoder()\n",
    "y = lbl_enc.fit_transform(train.author.values)\n",
    "print(y[:50])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5744703",
   "metadata": {},
   "source": [
    "## V Split the dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2b8b1f00",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(17621,)\n",
      "(1958,)\n"
     ]
    }
   ],
   "source": [
    "x_train, x_valid, y_train, y_valid = train_test_split(train.text.values, y, stratify=y, random_state=42, shuffle=True, test_size=0.1)\n",
    "print(x_train.shape)\n",
    "print(x_valid.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8116ab11",
   "metadata": {},
   "source": [
    "## VI Building basic models\n",
    "\n",
    "I will start with the Logistic Regression using term-frequency - inverse-document-frequency features. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "583fa45c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(17621, 15102)\n"
     ]
    }
   ],
   "source": [
    "tfv = TfidfVectorizer(min_df=3, max_features=None, strip_accents='unicode', \n",
    "analyzer='word', token_pattern=r'\\w{1,}', ngram_range=(1, 3), \n",
    "use_idf=1, smooth_idf=1, sublinear_tf=1, stop_words='english')\n",
    "\n",
    "tfv.fit(list(x_train) + list(x_valid))\n",
    "\n",
    "x_train_tfv = tfv.transform(x_train)\n",
    "x_valid_tfv = tfv.transform(x_valid)\n",
    "\n",
    "print(x_train_tfv.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "54bc2c17",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.6943152  0.07172106 0.23396374]\n",
      " [0.79796084 0.08133734 0.12070181]\n",
      " [0.61024582 0.16350201 0.22625217]\n",
      " ...\n",
      " [0.30012424 0.25166445 0.44821132]\n",
      " [0.20335046 0.16891225 0.6277373 ]\n",
      " [0.05947416 0.90771653 0.03280932]]\n",
      "logloss: 0.572 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/tavchija/.local/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    }
   ],
   "source": [
    "# Fitting a simple Logistic Regression on TFIDF\n",
    "clf = LogisticRegression()\n",
    "clf.fit(x_train_tfv, y_train)\n",
    "predictions = clf.predict_proba(x_valid_tfv)\n",
    "\n",
    "print(predictions)\n",
    "\n",
    "print(\"logloss: %0.3f \" % multiclass_logloss(y_valid, predictions))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "77f96126",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(17621, 400266)\n"
     ]
    }
   ],
   "source": [
    "# Now using a count vector\n",
    "ctv = CountVectorizer(analyzer='word', token_pattern=r'\\w{1,}', \n",
    "ngram_range=(1, 3), stop_words = 'english')\n",
    "\n",
    "ctv.fit(list(x_train) + list(x_valid))\n",
    "x_train_ctv = ctv.transform(x_train)\n",
    "x_valid_ctv = ctv.transform(x_valid)\n",
    "\n",
    "print(x_train_ctv.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "9a1a4ca5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "logloss: 0.527 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/tavchija/.local/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    }
   ],
   "source": [
    "# Now fitting to a simple Logistic Regrassion\n",
    "clf = LogisticRegression()\n",
    "clf.fit(x_train_ctv, y_train)\n",
    "predictions = clf.predict_proba(x_valid_ctv)\n",
    "\n",
    "print (\"logloss: %0.3f \" % multiclass_logloss(y_valid, predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "5f5da7a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "logloss: 0.578 \n"
     ]
    }
   ],
   "source": [
    "# Fitting a simple Naive Bayes on TFIDF\n",
    "clf = MultinomialNB()\n",
    "clf.fit(x_train_tfv, y_train)\n",
    "predictions = clf.predict_proba(x_valid_tfv)\n",
    "\n",
    "print (\"logloss: %0.3f \" % multiclass_logloss(y_valid, predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "45db6e0e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "logloss: 0.485 \n"
     ]
    }
   ],
   "source": [
    "clf = MultinomialNB()\n",
    "clf.fit(x_train_ctv, y_train)\n",
    "predictions = clf.predict_proba(x_valid_ctv)\n",
    "\n",
    "print (\"logloss: %0.3f \" % multiclass_logloss(y_valid, predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6292b425",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now I will try SVM, but first I will reduce da dimenisonality with singular valued decomposition\n",
    "svd = decomposition.TruncatedSVD(n_components=120)\n",
    "svd.fit(x_train_tfv)\n",
    "\n",
    "x_train_svd = svd.transform(x_train_tfv)\n",
    "x_valid_svd = svd.transform(x_valid_tfv)\n",
    "\n",
    "scl = preprocessing.StandardScaler()\n",
    "scl.fit(x_train_svd)\n",
    "x_train_svd_scl = scl.transform(x_train_svd)\n",
    "x_valid_svd_scl = scl.transform(x_valid_svd)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "a0b8bde1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "logloss: 0.735 \n"
     ]
    }
   ],
   "source": [
    "# Fitting a simple SVM\n",
    "clf = SVC(probability=True)\n",
    "clf.fit(x_train_svd_scl, y_train)\n",
    "predictions = clf.predict_proba(x_valid_svd_scl)\n",
    "\n",
    "print (\"logloss: %0.3f \" % multiclass_logloss(y_valid, predictions))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "598ae014",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/tavchija/.local/lib/python3.8/site-packages/xgboost/sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[18:53:21] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"learning_rat\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[18:53:22] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "logloss: 0.673 \n"
     ]
    }
   ],
   "source": [
    "clf = xgb.XGBClassifier(max_depth=7, n_estimators=200, colsample_bytree=0.8,\n",
    "subsample=0.8, nthread=10, learning_rat=0.1)\n",
    "\n",
    "clf.fit(x_train_tfv.tocsc(), y_train)\n",
    "predictions = clf.predict_proba(x_valid_tfv.tocsc())\n",
    "\n",
    "print(\"logloss: %0.3f \" % multiclass_logloss(y_valid, predictions))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84269bac",
   "metadata": {},
   "source": [
    "## VII Grid Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "dd8a5dd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "mll_scorer = metrics.make_scorer(multiclass_logloss, greater_is_better=False, needs_proba=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b1ed8c7",
   "metadata": {},
   "source": [
    "Logistic Regression in pipeline with svd and scl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "a479bff7",
   "metadata": {},
   "outputs": [],
   "source": [
    "svd = TruncatedSVD()\n",
    "\n",
    "scl = preprocessing.StandardScaler()\n",
    "\n",
    "lr_model = LogisticRegression()\n",
    "\n",
    "clf = pipeline.Pipeline([\n",
    "    ('svd', svd),\n",
    "    ('scl', scl),\n",
    "    ('lr', lr_model)\n",
    "])\n",
    "\n",
    "param_grid = {\n",
    "    'svd__n_components' : [120, 180],\n",
    "    'lr__C' : [0.1, 1.0, 10],\n",
    "    'lr__penalty' : ['l1', 'l2']\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e47cca77",
   "metadata": {},
   "source": [
    "Now start the Grid Search model for tunning hyperparameters for logistic regression model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "391213ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 2 folds for each of 12 candidates, totalling 24 fits\n",
      "[CV 1/2; 1/12] START lr__C=0.1, lr__penalty=l1, svd__n_components=120...........\n",
      "[CV 1/2; 2/12] START lr__C=0.1, lr__penalty=l1, svd__n_components=180...........\n",
      "[CV 1/2; 3/12] START lr__C=0.1, lr__penalty=l2, svd__n_components=120...........\n",
      "[CV 2/2; 1/12] START lr__C=0.1, lr__penalty=l1, svd__n_components=120...........\n",
      "[CV 2/2; 2/12] START lr__C=0.1, lr__penalty=l1, svd__n_components=180...........\n",
      "[CV 2/2; 3/12] START lr__C=0.1, lr__penalty=l2, svd__n_components=120...........\n",
      "[CV 2/2; 5/12] START lr__C=1.0, lr__penalty=l1, svd__n_components=120...........\n",
      "[CV 2/2; 4/12] START lr__C=0.1, lr__penalty=l2, svd__n_components=180...........\n",
      "[CV 1/2; 4/12] START lr__C=0.1, lr__penalty=l2, svd__n_components=180...........\n",
      "[CV 2/2; 6/12] START lr__C=1.0, lr__penalty=l1, svd__n_components=180...........\n",
      "[CV 1/2; 7/12] START lr__C=1.0, lr__penalty=l2, svd__n_components=120...........\n",
      "[CV 2/2; 7/12] START lr__C=1.0, lr__penalty=l2, svd__n_components=120...........\n",
      "[CV 1/2; 8/12] START lr__C=1.0, lr__penalty=l2, svd__n_components=180...........\n",
      "[CV 1/2; 6/12] START lr__C=1.0, lr__penalty=l1, svd__n_components=180...........\n",
      "[CV 1/2; 5/12] START lr__C=1.0, lr__penalty=l1, svd__n_components=120...........\n",
      "[CV 2/2; 8/12] START lr__C=1.0, lr__penalty=l2, svd__n_components=180...........\n",
      "[CV 2/2; 1/12] END lr__C=0.1, lr__penalty=l1, svd__n_components=120;, score=nan total time=   7.9s\n",
      "[CV 2/2; 5/12] END lr__C=1.0, lr__penalty=l1, svd__n_components=120;, score=nan total time=   7.9s\n",
      "[CV 1/2; 9/12] START lr__C=10, lr__penalty=l1, svd__n_components=120............\n",
      "[CV 1/2; 1/12] END lr__C=0.1, lr__penalty=l1, svd__n_components=120;, score=nan total time=   8.2s\n",
      "[CV 2/2; 9/12] START lr__C=10, lr__penalty=l1, svd__n_components=120............\n",
      "[CV 1/2; 10/12] START lr__C=10, lr__penalty=l1, svd__n_components=180...........\n",
      "[CV 1/2; 5/12] END lr__C=1.0, lr__penalty=l1, svd__n_components=120;, score=nan total time=   7.9s\n",
      "[CV 2/2; 10/12] START lr__C=10, lr__penalty=l1, svd__n_components=180...........\n",
      "[CV 2/2; 3/12] END lr__C=0.1, lr__penalty=l2, svd__n_components=120;, score=-0.766 total time=   8.7s\n",
      "[CV 1/2; 3/12] END lr__C=0.1, lr__penalty=l2, svd__n_components=120;, score=-0.781 total time=   8.8s\n",
      "[CV 1/2; 11/12] START lr__C=10, lr__penalty=l2, svd__n_components=120...........\n",
      "[CV 2/2; 11/12] START lr__C=10, lr__penalty=l2, svd__n_components=120...........\n",
      "[CV 1/2; 7/12] END lr__C=1.0, lr__penalty=l2, svd__n_components=120;, score=-0.778 total time=   8.8s\n",
      "[CV 1/2; 12/12] START lr__C=10, lr__penalty=l2, svd__n_components=180...........\n",
      "[CV 2/2; 7/12] END lr__C=1.0, lr__penalty=l2, svd__n_components=120;, score=-0.762 total time=  10.1s\n",
      "[CV 2/2; 12/12] START lr__C=10, lr__penalty=l2, svd__n_components=180...........\n",
      "[CV 2/2; 2/12] END lr__C=0.1, lr__penalty=l1, svd__n_components=180;, score=nan total time=  12.4s\n",
      "[CV 2/2; 6/12] END lr__C=1.0, lr__penalty=l1, svd__n_components=180;, score=nan total time=  12.5s\n",
      "[CV 1/2; 2/12] END lr__C=0.1, lr__penalty=l1, svd__n_components=180;, score=nan total time=  13.1s\n",
      "[CV 1/2; 6/12] END lr__C=1.0, lr__penalty=l1, svd__n_components=180;, score=nan total time=  12.8s\n",
      "[CV 2/2; 4/12] END lr__C=0.1, lr__penalty=l2, svd__n_components=180;, score=-0.731 total time=  13.0s\n",
      "[CV 1/2; 8/12] END lr__C=1.0, lr__penalty=l2, svd__n_components=180;, score=-0.736 total time=  13.7s\n",
      "[CV 2/2; 8/12] END lr__C=1.0, lr__penalty=l2, svd__n_components=180;, score=-0.728 total time=  13.4s\n",
      "[CV 1/2; 4/12] END lr__C=0.1, lr__penalty=l2, svd__n_components=180;, score=-0.742 total time=  13.9s\n",
      "[CV 1/2; 9/12] END lr__C=10, lr__penalty=l1, svd__n_components=120;, score=nan total time=   6.3s\n",
      "[CV 2/2; 9/12] END lr__C=10, lr__penalty=l1, svd__n_components=120;, score=nan total time=   6.6s\n",
      "[CV 2/2; 11/12] END lr__C=10, lr__penalty=l2, svd__n_components=120;, score=-0.773 total time=   6.4s\n",
      "[CV 1/2; 11/12] END lr__C=10, lr__penalty=l2, svd__n_components=120;, score=-0.770 total time=   6.5s\n",
      "[CV 1/2; 10/12] END lr__C=10, lr__penalty=l1, svd__n_components=180;, score=nan total time=   7.3s\n",
      "[CV 2/2; 10/12] END lr__C=10, lr__penalty=l1, svd__n_components=180;, score=nan total time=   7.2s\n",
      "[CV 1/2; 12/12] END lr__C=10, lr__penalty=l2, svd__n_components=180;, score=-0.748 total time=   6.9s\n",
      "[CV 2/2; 12/12] END lr__C=10, lr__penalty=l2, svd__n_components=180;, score=-0.733 total time=   5.6s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/tavchija/.local/lib/python3.8/site-packages/sklearn/model_selection/_validation.py:372: FitFailedWarning: \n",
      "12 fits failed out of a total of 24.\n",
      "The score on these train-test partitions for these parameters will be set to nan.\n",
      "If these failures are not expected, you can try to debug them by setting error_score='raise'.\n",
      "\n",
      "Below are more details about the failures:\n",
      "--------------------------------------------------------------------------------\n",
      "12 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/tavchija/.local/lib/python3.8/site-packages/sklearn/model_selection/_validation.py\", line 681, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"/home/tavchija/.local/lib/python3.8/site-packages/sklearn/pipeline.py\", line 394, in fit\n",
      "    self._final_estimator.fit(Xt, y, **fit_params_last_step)\n",
      "  File \"/home/tavchija/.local/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py\", line 1461, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"/home/tavchija/.local/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py\", line 447, in _check_solver\n",
      "    raise ValueError(\n",
      "ValueError: Solver lbfgs supports only 'l2' or 'none' penalties, got l1 penalty.\n",
      "\n",
      "  warnings.warn(some_fits_failed_message, FitFailedWarning)\n",
      "/home/tavchija/.local/lib/python3.8/site-packages/sklearn/model_selection/_search.py:969: UserWarning: One or more of the test scores are non-finite: [        nan         nan -0.77320695 -0.73686091         nan         nan\n",
      " -0.77006242 -0.7323014          nan         nan -0.77164259 -0.74013501]\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best score: -0.732\n",
      "Best parameters set:\n",
      "\tlr__C: 1.0\n",
      "\tlr__penalty: 'l2'\n",
      "\tsvd__n_components: 180\n"
     ]
    }
   ],
   "source": [
    "model = GridSearchCV(\n",
    "    estimator=clf,\n",
    "    param_grid=param_grid,\n",
    "    scoring=mll_scorer,\n",
    "    verbose=10,\n",
    "    n_jobs=-1,\n",
    "    refit=True,\n",
    "    cv=2\n",
    ")\n",
    "\n",
    "model.fit(x_train_tfv, y_train)\n",
    "\n",
    "print(\"Best score: %0.3f\" % model.best_score_)\n",
    "print(\"Best parameters set:\")\n",
    "best_parameters = model.best_estimator_.get_params()\n",
    "for param_name in sorted(param_grid.keys()):\n",
    "    print(\"\\t%s: %r\" % (param_name, best_parameters[param_name]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "1afc326d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 2 folds for each of 6 candidates, totalling 12 fits\n",
      "[CV 1/2; 2/6] START nb__alpha=0.01..............................................\n",
      "[CV 1/2; 2/6] END ..............nb__alpha=0.01;, score=-0.511 total time=   0.0s\n",
      "[CV 2/2; 1/6] START nb__alpha=0.001.............................................\n",
      "[CV 1/2; 1/6] START nb__alpha=0.001.............................................\n",
      "[CV 2/2; 2/6] START nb__alpha=0.01..............................................\n",
      "[CV 1/2; 4/6] START nb__alpha=1.................................................\n",
      "[CV 2/2; 1/6] END .............nb__alpha=0.001;, score=-0.641 total time=   0.0s\n",
      "[CV 1/2; 1/6] END .............nb__alpha=0.001;, score=-0.620 total time=   0.0s\n",
      "[CV 2/2; 2/6] END ..............nb__alpha=0.01;, score=-0.523 total time=   0.0s\n",
      "[CV 1/2; 4/6] END .................nb__alpha=1;, score=-0.663 total time=   0.0s\n",
      "[CV 1/2; 3/6] START nb__alpha=0.1...............................................\n",
      "[CV 2/2; 3/6] START nb__alpha=0.1...............................................\n",
      "[CV 1/2; 3/6] END ...............nb__alpha=0.1;, score=-0.489 total time=   0.0s\n",
      "[CV 2/2; 3/6] END ...............nb__alpha=0.1;, score=-0.495 total time=   0.0s\n",
      "[CV 1/2; 5/6] START nb__alpha=10................................................\n",
      "[CV 1/2; 5/6] END ................nb__alpha=10;, score=-0.950 total time=   0.0s\n",
      "[CV 2/2; 5/6] START nb__alpha=10................................................\n",
      "[CV 2/2; 4/6] START nb__alpha=1.................................................\n",
      "[CV 2/2; 4/6] END .................nb__alpha=1;, score=-0.666 total time=   0.0s\n",
      "[CV 2/2; 5/6] END ................nb__alpha=10;, score=-0.951 total time=   0.0s\n",
      "[CV 1/2; 6/6] START nb__alpha=100...............................................\n",
      "[CV 1/2; 6/6] END ...............nb__alpha=100;, score=-1.067 total time=   0.0s\n",
      "[CV 2/2; 6/6] START nb__alpha=100...............................................\n",
      "[CV 2/2; 6/6] END ...............nb__alpha=100;, score=-1.067 total time=   0.0s\n",
      "Best score: -0.492\n",
      "Best parameters set:\n",
      "\tnb__alpha: 0.1\n"
     ]
    }
   ],
   "source": [
    "# The same thing now using naive bayes model\n",
    "\n",
    "nb_model = MultinomialNB()\n",
    "\n",
    "clf = pipeline.Pipeline([('nb', nb_model)])\n",
    "\n",
    "param_grid = {\n",
    "    'nb__alpha' : [0.001, 0.01, 0.1, 1, 10, 100]\n",
    "}\n",
    "\n",
    "model = GridSearchCV(\n",
    "    estimator=clf,\n",
    "    param_grid=param_grid, \n",
    "    scoring=mll_scorer, \n",
    "    verbose=10, \n",
    "    n_jobs=-1, \n",
    "    refit=True, \n",
    "    cv=2\n",
    ")\n",
    "\n",
    "model.fit(x_train_tfv, y_train)\n",
    "\n",
    "print(\"Best score: %0.3f\" % model.best_score_)\n",
    "print(\"Best parameters set:\")\n",
    "best_parameters = model.best_estimator_.get_params()\n",
    "for param_name in sorted(param_grid.keys()):\n",
    "    print(\"\\t%s: %r\" % (param_name, best_parameters[param_name]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d11612c",
   "metadata": {},
   "source": [
    "## VII Word Vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5c5c346f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_float(element):\n",
    "    try:\n",
    "        float(element)\n",
    "        return True\n",
    "    except ValueError:\n",
    "        return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7f800eca",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'tqdm' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_10688/3566922403.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/home/tavchija/Documents/Data-Science/embeddings/glove.840B.300d.txt'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0;32mfor\u001b[0m \u001b[0mline\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtqdm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m     \u001b[0mvalues\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mline\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0mword\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'tqdm' is not defined"
     ]
    }
   ],
   "source": [
    "embeddings_index = {}\n",
    "f = open('/home/tavchija/Documents/Data-Science/embeddings/glove.840B.300d.txt')\n",
    "\n",
    "for line in tqdm(f):\n",
    "    values = line.split()\n",
    "    word = \"\"\n",
    "    i = 0\n",
    "    while is_float(values[i]) == False:\n",
    "        word += \" \" + values[i]\n",
    "        i+=1\n",
    "    \n",
    "    coefs = np.asarray(values[i:], dtype='float32')\n",
    "    embeddings_index[word] = coefs\n",
    "f.close()\n",
    "\n",
    "print('Found %s word vectors.' % len(embeddings_index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d80d3057",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sent2vec(s):\n",
    "    words = str(s).lower().decode('utf-8')\n",
    "    words = word_tokenize(words)\n",
    "    words = [w for w in words if not w in stop_words]\n",
    "    words = [w for w in words if w.isalpha()]\n",
    "    M = []\n",
    "    for w in words:\n",
    "        try:\n",
    "            M.append(embeddings_index[w])\n",
    "        except:\n",
    "            continue\n",
    "    M = np.array(M)\n",
    "    v = M.sum(axis=0)\n",
    "    if type(v) != np.ndarray:\n",
    "        return np.zeros(300)\n",
    "    return v / np.sqrt((v ** 2).sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66bf3c78",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
